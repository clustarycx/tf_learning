Ringreducer 
 
 
Coding standards  
 
https://www.tensorflow.org/community/style_guide 
 
 
http://www.sohu.com/a/127596575_494939 
 
 
Other libs 
 
Gloo provides ring allreduce and bcube allreduce.  
 
Gloo is a collective communications library. It comes with a number of collective algorithms useful for machine learning applications. These include a barrier, broadcast, and allreduce. 
  
Transport of data between participating machines is abstracted so that IP can be used at all times, or InifiniBand (or RoCE) when available. In the latter case, if the InfiniBand transport is used, GPUDirect can be used to accelerate cross machine GPU-to-GPU memory transfers. 
 
https://github.com/facebookincubator/gloo 
 
 
Horovod: fast and easy distributed deep learning in TensorFlow 
  
from Uber 
  
. We replaced the Baidu ring-allreduce implementation with NCCL [13]. NCCL is NVIDIA’s 
library for collective communication that provides a highly optimized version of ringallreduce. 
NCCL 2 introduced the ability to run ring-allreduce across multiple machines, 
enabling us to take advantage of its many performance boosting optimizations 
  
  
https://eng.uber.com/horovod/ 
https://github.com/uber/horovod 
 
 
NCCL 
 
NCCL already include ringreduce 
 
https://www.tensorflow.org/api_docs/python/tf/contrib/nccl 
 
In distributed environment, need to use mpi, to  
 
 
 
 
 
 
There are three Ops which use Ringreducer 
 
CollectiveReduce 
CollectiveBcastSend 
CollectiveBcastRecv 
 
OpKernel 
    AsyncOpKernel 
        CollectiveOpKernel 
            CollectiveBcastRecvOpKernel 
            CollectiveBcastSendOpKernel 
            CollectiveReduceOpKernel 
 
The 3 Ops are registered like this: 
  
REGISTER_OP("CollectiveReduce") 
    .Input("input: T") 
    .Output("data: T") 
    .Attr("T: {float, float16, float64, int32, int64}") 
    .Attr("group_size: int") 
    .Attr("group_key: int") 
    .Attr("instance_key: int") 
    .Attr("merge_op: {'Min', 'Max', 'Mul', 'Add'}") 
    .Attr("final_op: {'Id', 'Div'}") 
    .Attr("subdiv_offsets: list(int)") 
    .SetIsStateful() 
    .SetShapeFn(shape_inference::UnchangedShape); 
  
REGISTER_OP("CollectiveBcastSend") 
    .Input("input: T") 
    .Output("data: T") 
    .Attr("T: {float, float16, float64, int32, int64}") 
    .Attr("group_size: int") 
    .Attr("group_key: int") 
    .Attr("instance_key: int") 
    .Attr("shape: shape") 
    .SetIsStateful() 
    .SetShapeFn(shape_inference::ExplicitShape); 
  
REGISTER_OP("CollectiveBcastRecv") 
    .Output("data: T") 
    .Attr("T: {float, float16, float64, int32, int64}") 
    .Attr("group_size: int") 
    .Attr("group_key: int") 
    .Attr("instance_key: int") 
    .Attr("shape: shape") 
    .SetIsStateful() 
    .SetShapeFn(shape_inference::ExplicitShape); 
  
}  // namespace tensorflow 
 
 
 AsyncOpKernel::Compute will call  AsyncOpKernel::ComputeAsync, which is a virtual function, will call the real implementation accordingly. 
 
CollectiveBcastRecvOpKernel::ComputeAsync 
CollectiveBcastSendOpKernel::ComputeAsync 
CollectiveReduceOpKernel::ComputeAsync 
 
Theses functions will call BaseCollectiveExecutor::ExecuteAsync in this way: 
 
CollectiveExecutor* col_exec = c->collective_executor(); 
col_exec->ExecuteAsync(c, col_params_, GetCollectiveKey(c), actual_done); 
 
 
BaseCollectiveExecutor::ExecuteAsync wil use RingReducer like this: 
 
RingReducer* reducer = 
          CreateReducer(ctx, CtxParams(ctx), col_params, exec_key, step_id_, 
                        input, output, &error); 
 
      SchedClosure([reducer, done_safe]() { 
        reducer->Run([reducer, done_safe](const Status& s) { 
          done_safe(s); 
          delete reducer; 
        }); 
      }); 
 
 
RingReducer::DispatchSend   
RingReducer::DispatchRecv 
  
called by :  
  
    RingReducer::RunAsyncParts 
  
called by: 
  
        RingReducer::ContinueAfterInputCopy 
  
called by: 
  
            RingReducer::Run 
 
 
CollectiveExecutor* col_exec_; 
 
col_exec_->RecvFromPeer 
col_exec_->PostToPeer 
 
Are used in  
 
RingReducer::DispatchSend  
RingReducer::DispatchRecv 
 
Here is the class relationship: 
 
PeerAccessInterface 
core::RefCounted 
    CollectiveExecutor 
        BaseCollectiveExecutor 
      
PeerAccessInterface 
DeviceResolverInterface 
    CollectiveRemoteAccess 
        PerStepCollectiveRemoteAccess 
            CollectiveRemoteAccessLocal 
                CollectiveRemoteAccessDistributed 
                 
RecvFromPeer and PostToPeer    are PeerAccessInterface's virtual methods. 
             
PeerAccessInterface::RecvFromPeer 
PeerAccessInterface::PostToPeer 
 
CollectiveRemoteAccessLocal and CollectiveRemoteAccessDistributed are the implement of Post and Recv 
 
CollectiveRemoteAccessLocal::PostToPeer 
BufRendezvous buf_rendezvous_; 
buf_rendezvous_.ProvideBuf 
  
CollectiveRemoteAccessLocal::RecvFromPeer 
BufRendezvous buf_rendezvous_; 
buf_rendezvous_.ConsumeBuf 
  
  
CollectiveRemoteAccessDistributed::RecvFromPeer 
 
Basically, for GPU dest: 
it copies the input tensor to CPU, and then copies it to GPU. 
memcpy(DMAHelper::base(cpu_tensor), extra.tensor_content().data(),num_bytes); 
CopyTensor::ViaDMA 
  
 
for CPU dest: 
copy it once 
CopyTensor::ViaDMA 
 
 
 
 
So, the questions here are 
 
How to use GDR to do the copy? 
Will NCCL be a solution? 
 
In BaseCollectiveExecutor::ExecuteAsync 
  
      // TODO(tucker): support other reduction algorithms, 
      // e.g. tree-reduce, hybrid tree/ring, delegate-to-NCCL, etc. 
  
This will compete with our GDR solution? 
 
Is NCCL good enough to be used in distributed deployment? 
 
 
 
 
Another 2 questions: 
 
And both Post and Recv use Tensor* as a parameter, but Recv and Post use RingField as parameter, what is the relationship between Tensor and RingField? 
 
For Recv 
  RingField* rf 
  Tensor* dst_tensor = (!rf->second_pass && (col_params_.merge_op != nullptr)) 
                           ? &rf->tmp_chunk 
                           : &rf->chunk; 
 
For Post 
It uses &rf->chunk, where RingField* rf 
 
 
rf->chunk = ca_->ChunkAlias(rf->sc_idx); 
 
 
CollectiveAdapter:: 
  // Returns a new Tensor that aliases the required chunk. 
  Tensor ChunkAlias(int i) override { 
    int64 start = chunk_elts_ * i; 
    int64 num_elts = ChunkElts(i); 
    // If this chunk is empty the prior chunk might also be short 
    // so always take an empty slice from the front of the tensor 
    // to avoid an illegal offset check failure somewhere. 
    return (num_elts > 0) ? output_.Slice(start, start + num_elts) 
                          : output_.Slice(0, 0); 
  } 
 
Note: it’s using the same buffer, not creating new ones. Just returns the offset. 
 
 
 
And in RingReducer::Run, CollectiveRemoteAccessLocal::MemCpyAsync on input_ and output_ is called, but why need to copy it to output_ ? 
 
RingReducer::RunAsyncParts,  what does num_subdivs_ used for? 
rfv_.resize(group_size_ * num_subdivs_); 
 
 
The meaning of group_size_ and num_subdivs_  : 
 
// At the beginning of the algorithm initialize a RingField struct for 
// every independent field of the tensor. 
void RingReducer::InitRingField 
  
// Note on field indexing: There are group_size_ devices in the 
  // instance, implying the same number of chunks per tensor, where a 
  // chunk is the unit of data transferred in a time step.  However, if 
  // a device can simultaneously send data by 2 or more independent 
  // channels we can speed up the transfer by subdividing chunks and 
  // processing multiple subdivisions at once.  So the actual number 
  // of RingFields is group_size_ * num_subdivs_. 
 
 
How are the collectives Ops used? 
 
CollectiveExecutor::ExecuteAsync is used by   
GraphMgr::ExecuteAsync 
 
Like this: 
 
RemoteRendezvous* rendezvous = worker_env_->rendezvous_mgr->Find(step_id); 
  Status s = rendezvous->Initialize(session); 
  CollectiveExecutor::Handle* ce_handle = 
      item->collective_graph_key != BuildGraphOptions::kNoCollectiveGraphKey 
          ? new CollectiveExecutor::Handle( 
                worker_env_->collective_executor_mgr->FindOrCreate(step_id), 
                true) 
          : nullptr; 
 
… 
 
  StartParallelExecutors(handle, step_id, item, rendezvous, ce_handle, 
                         collector, cost_graph, cancellation_manager, 
                         [item, rendezvous, ce_handle, done](const Status& s) { 
                           done(s); 
                           rendezvous->Unref(); 
                           item->Unref(); 
                           delete ce_handle; 
                         }); 
 
Collective_executor will be used when 
 item->collective_graph_key != BuildGraphOptions::kNoCollectiveGraphKey 
 
 
In MasterSession::DoPartialRun 
 
  // CollectiveOps are not supported in partial runs. 
  if (req.options().experimental().collective_graph_key() != 
      BuildGraphOptions::kNoCollectiveGraphKey) { 
    return errors::InvalidArgument( 
        "PartialRun does not support Collective ops.  collective_graph_key " 
        "must be kNoCollectiveGraphKey."); 
  } 
 
 
 
What kind of use case will fulfil this condition? 
 
 
In  tensorflow/python/ops/collective_ops_test.py 
 
from __future__ import absolute_import 
from __future__ import division 
from __future__ import print_function 
  
from tensorflow.core.protobuf import config_pb2 
from tensorflow.python.framework import constant_op 
from tensorflow.python.framework import ops 
from tensorflow.python.ops import collective_ops 
from tensorflow.python.platform import test 
 
class CollectiveOpTest(test.TestCase): 
  
  def _testCollectiveReduce(self, t0, t1, expected): 
    group_key = 1 
    instance_key = 1 
    with self.test_session( 
        config=config_pb2.ConfigProto(device_count={'CPU': 2})) as sess: 
      with ops.device('/CPU:0'): 
        in0 = constant_op.constant(t0) 
        colred0 = collective_ops.all_reduce(in0, 2, group_key, instance_key, 
                                            'Add', 'Div') 
      with ops.device('/CPU:1'): 
        in1 = constant_op.constant(t1) 
        colred1 = collective_ops.all_reduce(in1, 2, group_key, instance_key, 
                                            'Add', 'Div') 
      run_options = config_pb2.RunOptions() 
      run_options.experimental.collective_graph_key = 1 
      results = sess.run([colred0, colred1], options=run_options) 
    self.assertAllClose(results[0], expected, rtol=1e-5, atol=1e-5) 
    self.assertAllClose(results[1], expected, rtol=1e-5, atol=1e-5) 
  
  def testCollectiveReduce(self): 
    self._testCollectiveReduce([0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1], 
                               [0.3, 1.3, 2.3, 3.3, 4.3, 5.3, 6.3, 7.3], 
                               [0.2, 1.2, 2.2, 3.2, 4.2, 5.2, 6.2, 7.2]) 
  
  def _testCollectiveBroadcast(self, t0): 
    group_key = 1 
    instance_key = 1 
    with self.test_session( 
        config=config_pb2.ConfigProto(device_count={'CPU': 2})) as sess: 
      with ops.device('/CPU:0'): 
        in0 = constant_op.constant(t0) 
        out0 = collective_ops.broadcast_send(in0, in0.shape, in0.dtype, 
                                             2, group_key, instance_key) 
      with ops.device('/CPU:1'): 
        c1 = constant_op.constant(t0) 
        out1 = collective_ops.broadcast_recv(c1.shape, c1.dtype, 
                                             2, group_key, instance_key) 
      run_options = config_pb2.RunOptions() 
      run_options.experimental.collective_graph_key = 1 
      results = sess.run([out0, out1], options=run_options) 
    self.assertAllClose(results[0], t0, rtol=1e-5, atol=1e-5) 
    self.assertAllClose(results[1], t0, rtol=1e-5, atol=1e-5) 
  
  def testCollectiveBroadcast(self): 
    self._testCollectiveBroadcast([0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1]) 
  
  
if __name__ == '__main__': 
  test.main() 
 
 
Tensorflow/python/ops/collective_ops.py 
 
Three funcions: 
 
 
def all_reduce(t, group_size, group_key, instance_key, merge_op, final_op, 
               subdiv_offsets=(0,)): 
  """Reduces tensors collectively, across devices. 
  
  Args: 
    t: the tensor to be reduced. 
    group_size: the total number of tensors to be collectively reduced. 
      Each must reside on a different device. 
    group_key: an integer identifying the group of devices. 
    instance_key: an integer identifying the participating group of Ops. 
    merge_op: string naming the binary Op to be applied to compute each 
      partial reduction. 
    final_op: string naming the unary Op to be applied to each fully 
      reduced value.  Can be 'Id' for no operation. 
    subdiv_offsets: a list of integer offsets into the tensor at which each 
      independent subdivision should begin.  Use [0] if no subdivision should 
      be done. 
  
  Returns: 
    An Op implementing the distributed reduction. 
  
  Raises: 
    ValueError: if any of the input parameter constraints are not met. 
  """ 
 
  
def broadcast_send(t, shape, dtype, group_size, group_key, instance_key): 
  """Broadcasts one tensor to a group of others, across devices. 
  
  Args: 
    t: the tensor to be sent. 
    shape: the shape of the tensor being sent, which must agree with t. 
    dtype: the type of the tensor being sent, which must agree with t. 
    group_size: one plus the number of receiving tensors, i.e. the total 
      number of devices participating.  Each tensor must reside on a 
      different device. 
    group_key: an integer identifying the group of devices. 
    instance_key: an integer identifying the participating group of Ops. 
  
  Returns: 
    An Op implementing the distributed broadcast send. 
  
  Raises: 
    ValueError: if any of the input parameter constraints are not met. 
  
  Note that the shape and dtype arguments appear redundant since they 
  should be obtainable from t.  The are two reasons for including 
  them.  First, the shape and type of tensors passed via broadcast must 
  be known ahead of time in their most specific form so that the receive 
  side can allocate memory for the operation and shape/type inference can 
  carry forward from there.  Including the same declarations on the 
  send side clarifies a commitment already made.  Secondly, having nearly 
  identical use syntax for send and receive sides may simplify tool-driven 
  generation of broadcast. 
  """ 
 
  
def broadcast_recv(shape, dtype, group_size, group_key, instance_key): 
  """Receives a broadcasts tensor, across devices. 
  
  Args: 
    shape: Shape of the tensor to be received. 
    dtype: Type of the tensor to be received. 
    group_size: one plus the number of receiving tensors, i.e. the total 
      number of devices participating.  Each tensor must reside on a 
      different device. 
    group_key: an integer identifying the group of devices. 
    instance_key: an integer identifying the participating group of Ops. 
  
  Returns: 
    An Op implementing the broadcast receive. 
  
  Raises: 
    ValueError: if any of the input parameter constraints are not met. 
  """ 
 
In compiling environment, 
 
./bazel-tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/ops/gen_collective_ops.py:def collective_reduce(input, group_size, group_key, instance_key, merge_op, final_op, subdiv_offsets, name=None): 
 
It calls 
 
_execute.record_gradient( 
      "CollectiveReduce", _inputs_flat, _attrs, _result, name) 
 
 
In tensorflow/core/api_def/python_api/api_def_CollectiveReduce.pbtxt 
 
op { 
  graph_op_name: "CollectiveReduce" 
  endpoint { 
    name: "collective.all_reduce" 
  } 
} 
 
 
 
 
